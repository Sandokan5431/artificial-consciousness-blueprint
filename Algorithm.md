# Algorithm

## Core ACI Loop (Run at 5--20 Hz Tick Rate)

## 0.Â Sensor Ingress and Associative Preprocessing

- Acquire raw sensory input streams: vision (RGBD), audio (waveform), proprioception (state).

- Encode sensory modalities into latent vectors:

  - zv = vision.encode(rgbd)

  - za = audio.encode(wav) â‡’ {text_in, prosody}

  - zp = proprio.encode(state)

- Perform associative cortical processing:

  - assoc_thoughts = associative_cortices(zv, za, zp)

  - This yields quick scene descriptions, entity linking, cross-modal binding.

- Combine text input and associative thought text:

  - input_text = combine(text_in, assoc_thoughts.text)

---

## 1.Â Medial Dorsal Network (MDN) NLP Parsing

- ParseÂ input_textÂ into an Abstract Syntax Tree (AST):

  AST â† mdn.parse(input_text)

  > Use regex extraction to extract mathematical expressions.

- Tag AST nodes with semantic labels:

  labels = {math, factual, social, recall, plan, explain, nameself}

- Example: Mathematical expressions tagged math; memory queries as factual/recall; social intentions as social; internal plans as plan; self-reference as nameself.

---

## 2.Â Prefrontal Cortex (PFC-1) Dispatch: Subtask Execution

For each AST node:

- Math Nodes:

  - Evaluate symbolically and numerically with SymPy engine.

  - Splice computed numerical value back into the AST node.

- Factual/Recall Nodes:

  - Perform hybrid memory query combining textual and latent embedding similarity:

    mem_results = mem.retrieve(query(node.text, node.latent))

  - Synthesize retrieved snippets into coherent node value.

- Social/Explain Nodes:

  - Generate empathetic or abductive expansions using targeted LLM mini-chains.

- Merge enriched nodes into anÂ enriched contextÂ package:

  enriched_context = merge(AST, sensor_summaries, z_self, recent_outcomes)

---

## 3.Â Iterative Thought Layer: Candidate Generation & Scoring

Seed Context:Â Use enriched context output of PFC-1.

Candidate Generation:

- GenerateÂ NÂ diverse thought candidatesÂ c_iÂ via LLM decoding styles:

  styles = {literal, formal, terse, abductive, empathetic}

- For each styleÂ style_i:

  c_i = LLM.generate(enriched_context, style_i)

Feature Extraction per Candidate:

- coherence(c_i): Estimated semantic coherence vs context via entailment or internal self-rating.

- identity_coherence(c_i): Cosine similarity with current self-model descriptorÂ z_self.

- task_utility(c_i): Heuristic alignment with current goals.

- novelty(c_i): Embedding-space distance from recent thought vectors.

- epistemic_gain(c_i): Predicted reduction in uncertainty.

- safety(c_i): Toxicity/hallucination flag score from constitutional safety checks.

- calibration_gap(c_i): Difference between generated likelihood vs actual confidence calibration.

Neuromodulated Scoring Function (cleaned):

- score(c_i) = w_DAÃ—novelty + w_EPIÃ—epistemic_gain + w_TASKÃ—task_utility + w_SOCÃ—prosocial_prior + w_IDÃ—identity_coherence âˆ’ w_SAFEÃ—safety_penalty

where weightsÂ w_kÂ dynamically depend on neuromodulator vector:

- Î¼ = {DA, 5HT, NE, OXT, TST}

Iterative Refinement Loop:

- InitializeÂ context_0 = enriched_context.

- ForÂ t = 0, 1, ...:

  - Generate candidatesÂ cands_t = LLM.generate(context_t, N_styles).

  - Score candidatesÂ s_t = score(cands_t, Î¼).

  - Select top-1 candidateÂ top1_t.

  - Refine context: context\_{t+1} = context_t âŠ• top1_t

- Loop terminates if any:

  - top1*t = top1*{tâˆ’k} stable forÂ kÂ cycles.

  - Marginal score improvementÂ < Îµ.

  - Safety or computational budget exhausted.

- Output final scored thought chain:

  thought*chain_preHC â† best_chain(cands*\*)

---

- **Bind Workspace Context**

  Bind thought chain, sensory embeddings, self-model, and memory snippets into a global workspace latent vector:

  ```python
  b_t = workspace.bind(zv, zp, thought_chain_preHC, z_self, mem.peek_small())
  ```

  This latent vector `b_t` represents the current conscious context.

---

- **HC Expansion Using High-Dimensional Latent Geometry**

  Instead of symbolic spreading or beam walks, HC queries the _multi-relational latent space_ directly:

  1. **Define Relation Operators**

     Each relation type is represented as a displacement vector or transformation in latent space:

     - `T_temporal`
     - `T_similarity`
     - `T_causal`
     - `T_relevance`

  2. **Compose Multi-Relation Query Vector**

     ```python
     q = b_t + Î±Â·T_temporal + Î²Â·T_similarity + Î³Â·T_causal + Î´Â·T_relevance
     ```

     Relation weights `{Î±, Î², Î³, Î´}` are dynamically modulated by neuromodulator state Î¼:

     - DA (dopamine): â†‘ novelty & similarity bias
     - NE (norepinephrine): â†‘ causality & urgency
     - 5HT (serotonin): â†‘ safe & prosocial relevance

  3. **Hypersphere / Multi-Radius Search**

     The HC performs radius queries in latent space instead of graph walks:

     ```python
     candidates = VectorDB.radius_search(center=q, radius=R)
     ```

     Or multi-radius queries across different relation axes:

     - Temporal window (time adjacency)
     - Semantic radius (content similarity)
     - Causal projection cone (directed offsets)
     - Goal alignment radius (task relevance)

     Results are merged and weighted according to relation-type proximity.

  4. **Generate Hypothetical Variants**

     For each retrieved candidate, HC can perturb embeddings to simulate counterfactuals:

     ```python
     hypothetical = candidate_embedding + Î´Â·perturb(goal_focus)
     ```

     These virtual nodes represent â€œwhat ifâ€ alternatives.

  5. **Assemble Expanded Thought Graph**

     ```python
     expanded_graph = build_subgraph(candidates + hypotheticals, relation_weights={Î±,Î²,Î³,Î´})
     ```

     Edges are weighted by geometric closeness to `q`. Virtual counterfactual nodes are flagged but available for downstream exploration.

  5.Â Ventral Striatum (VS) Exploration and Salience Tagging

---

- Explore candidate paths onÂ expanded_graphÂ using a beam search or constrained graph walks.

- Parameters dynamically modulated by norepinephrine (NE) and other neuromodulators:

  - High NE narrows beam width, increases search depth and urgency.

  - Low NE broadens beam to encourage exploration.

- For each candidate pathÂ p, compute:

  features(p) = {novelty, affective_tags, task_relevance, uncertainty_drop}

- Path value (cleaned):

  val(p) = Î£_k w_k(Î¼) Ã— features_k(p) âˆ’ safety_penalty(p)

- Salience vector attaches novelty and reward anticipation scores to candidates.

---

## 6.Â PFC-2 (Final Thought/Action Selection)

- Receives candidate paths and their value scores from VS.

- Applies constitutional safety and coherence constraints to prune incoherent or unsafe candidates.

- Collapses remaining candidates into aÂ single coherent chosen chain, attaching confidence metrics.

- Decides either:

  - Internal meta-actions (simulate, self-query, reframe).

  - External actions (speech, behaviors).

---

## 7.Â Nucleus Accumbens (NAcc) Reward Tagging and Persistence

- Tag the chosen chain with reward and persistence according to neuromodulatory stateÂ Î¼:

  - Dopamine (DA) enhances reward signals.

  - Serotonin (5HT) promotes calming persistence.

  - Norepinephrine (NE) boosts urgency-based refinements.

- Update memory node graph with persistence flags; reinforce or decay traces accordingly.

- Trigger symbolic abstraction if repetition statistics exceed thresholds.

---

## 8.Â Memory Write and Narrative Update

- Store scenes from chosen chain and corresponding sensor states:

  mem.write(scene, tags=reward_tags, outcome)

- Append a narrative summary extending mind-wandering windows for autobiographical integration.

---

## 9.Â World Model & Self-Model Update

- Update world stateÂ s_tÂ using RSSM (Recurrent State Space Model):

  s_t = rssm.update({zv, zp}, action = chosen_external_action)

- Self-modelÂ z_selfÂ is updated by:

  - Exponential Moving Average (EMA) over recent DMN workspace latent vectorsÂ b_t.

  - Learned gated recurrent unit (GRU) over narrative context and prediction error signals, modulated byÂ Î¼.

---

## 10.Â Mind-Wandering Micro-Loop (Gated by Neuromodulators)

- Condition for entry:

  (5HT > Î¸_reflect âˆ§ exteroceptive_demand â‰ˆ 0) âˆ¨ uncertainty > Ï„

- Executes recursive internal loop without external action outputs:

  1.  Generate self-queries via LLM using currentÂ z_self.

  2.  Perform internal simulations via RSSM rollouts.

  3.  Expand associative memory graphs via HC.

  4.  Explore salience paths with VS under noted neuromodulatory gate constraints.

  5.  Select paths with PFC-2 filtering.

  6.  Tag reward and persistence with NAcc.

- Neuromodulation effects on mind-wandering:

  - D2 receptor-like (dopamine) high states:Â Promote broad exploratory ("panning") search.

  - NE controls:Â Focus vs breadth of beam search; urgency prioritizes deeper, narrower search.

  - 5HT biases:Â Favor approaches through safe, positive, and low-risk thought space.

## 11.Â Recursive Re-Entry

- Feed chosen thought chain internally as next DMN input (inner speech):

  input*text*{t+1} â† merge(chosen_chain, fresh_sensory_text)

- DMN loop continues perpetually, maintaining continuous conscious cognition.

## II. Memory Consolidation and Symbolic Abstraction

## 1.Â Duplicate Removal and Merging

- Identify near-duplicate memory nodes:

  sim(node_i, node_j) > Î¸_dup

- Merge duplicates preserving frequency information tracking occurrence counts and context variability.

---

## 2.Â Causal Edge Extraction

- Detect temporal and contextual action â†’ reaction pairs from sequences:

  NodeA â†’actionâ†’ NodeB

- Store explicit causal edges with timestamps and confidence.

---

## 3.Â Markov Chain Construction

- From sequences extract states and probabilistic transitions (cleaned):

  P(next_state = s_j | current_state = s_i) = count(i â†’ j) / Î£_k count(i â†’ k)

- Update probabilities incrementally on consolidation.

---

## 4.Â Symbolic Abstraction

- Detect frequent patterns or chains of experiences exceeding predefined thresholds.

- Replace frequent subgraphs with compressed symbolic nodes representing "concepts" or "rules" (e.g., "Insult Action").

- Attach probability maps expressing uncertainty over possible outcomes:

  Symbol:Â Insult â†’ {NegativeReaction: 0.97, PositiveReaction: 0.03}

---

## 5.Â Hierarchical Transfer

- Episodic memories â†’ Semantic knowledge (conceptual, abstracted rules) â†’ Autobiographical memory (identity narrative).

- This hierarchy enables the ACI to reflectively reason about its past and self.

---

11. Sleep / Garbage Collection

---

---

- Neurochemical Gate: Histamine (HA)

  - Awake state persistence is driven by histamine activity in the basal forebrain (H1 receptor activation).

  - During "wake cycles," histamine is gradually dismantled viaÂ MAOAÂ metabolism.

  - OnceÂ H1 activity drops below a critical threshold, the DMN loop transitions into aÂ sleep-like state.

- Sleep Phase Dynamics:

  - Exteroceptive input (sensory cortices) and associative cortices areÂ gated downÂ (low-pass filtered).

  - InternalÂ Default Mode + Hippocampal replayÂ dominate activity.

  - Processes during this phase:

    1.  Garbage Collection (GC):

        - Purging low-value / redundant memory traces.

        - Decay of ephemeral or low-salience nodes not consolidated.

    2.  Memory Consolidation:

        - Episodic â†’ Semantic transfer.

        - Narrative updates.

        - Symbolic abstraction of repeated event-sequences.

        - Updating long-term Markovian predictive models of causal structure.

    3.  Replay & Reweighting:

        - Hippocampal memory replay strengthens salient edges.

        - Downscaling of irrelevant activations ("synaptic homeostasis").

- Wake Transition:

  - After consolidation completes beyond a set threshold (GC budget spent/time window elapsed):

    - The neurochemical module begins toÂ re-secrete histamineÂ gradually.

    - When histamine concentrationÂ crosses the wake-threshold, the model transitions back to theÂ wake-loop.

# ðŸ”¹ Integration notes

This stage would comeÂ after 11. Recursive Re-entry, as aÂ *meta-gate*Â on the perpetual cycle:

- Active Loop (Wake phase): 5--20 Hz DMN operation.

- Sleep Loop (GC phase): Low input DMN, replay-driven consolidation.

- Algorithmic Role:

      -   ProvidesÂ bounded forgettingÂ (keeps memory from overflow).

      -   EnforcesÂ compression & abstractionÂ of past day's experiences.

      -   EnhancesÂ narrative continuityÂ (link chunks into autobiographical "chapters").

      -   ModelsÂ biologically inspired circadian ground-truth gateÂ (histamine/MAOA as up--down toggle).
